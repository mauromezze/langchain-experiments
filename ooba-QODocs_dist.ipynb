{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wrapper around Oobabooga APIs.\"\"\"\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "import requests\n",
    "from pydantic import Extra, root_validator\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "\n",
    "VALID_TASKS = (\"text2text-generation\", \"text-generation\")\n",
    "\n",
    "class OobaboogaEndpoint(LLM):\n",
    "    \"\"\"Oobabooga text-generation-webui endpoints.\n",
    "    \"\"\"\n",
    "    \n",
    "    endpoint_url: str = \"\"\n",
    "    \"\"\"Endpoint URL to use.\"\"\"\n",
    "    task: Optional[str] = None\n",
    "    \"\"\"Task to call the model with. Should be a task that returns `generated_text`.\"\"\"\n",
    "    model_kwargs: Optional[dict] = None\n",
    "    \"\"\"Key word arguments to pass to the model.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        _model_kwargs = self.model_kwargs or {}\n",
    "        return {\n",
    "            **{\"endpoint_url\": self.endpoint_url, \"task\": self.task},\n",
    "            **{\"model_kwargs\": _model_kwargs},\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"oobabooga_endpoint\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call out to HuggingFace Hub's inference endpoint.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                response = hf(\"Tell me a joke.\")\n",
    "        \"\"\"\n",
    "\n",
    "        # send request\n",
    "        request = {\n",
    "            'prompt': prompt,\n",
    "            'max_new_tokens': 1024,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.1, #1.3\n",
    "            'top_p': 0.95, #0.1\n",
    "            'typical_p': 1,\n",
    "            'repetition_penalty': 1.0, #1.18\n",
    "            'top_k': 40,\n",
    "            'min_length': 0,\n",
    "            'no_repeat_ngram_size': 0,\n",
    "            'num_beams': 1,\n",
    "            'penalty_alpha': 0,\n",
    "            'length_penalty': 1,\n",
    "            'early_stopping': False,\n",
    "            'seed': -1,\n",
    "            'add_bos_token': True,\n",
    "            'truncation_length': 2048,\n",
    "            'ban_eos_token': False,\n",
    "            'skip_special_tokens': True,\n",
    "            'stopping_strings': [\"\\nObservation:\", \"\\n### Assistant:\", \"\\n### Human:\"] #[]\n",
    "        }\n",
    "\n",
    "        response = requests.post(URI, json=request)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()['results'][0]['text']\n",
    "            if result.rsplit(None, 1)[1] == (\"Observation\" or \"\\n### Human\" or \"\\n### Assistant\"):\n",
    "                result = result.rsplit(None, 1)[0]\n",
    "            else:\n",
    "                result\n",
    "        return result\n",
    "    \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "import os\n",
    "\n",
    "ABS_PATH = os.path.dirname(os.path.abspath(''))\n",
    "DB_DIR = os.path.join(ABS_PATH, \"db\")\n",
    "FILE_DIR = os.path.join(ABS_PATH, \"docs\")\n",
    "\n",
    "HOST = \"localhost:5000\"\n",
    "URI = f\"http://{HOST}/api/v1/generate\"\n",
    "\n",
    "endpoint_url = URI\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "llm = OobaboogaEndpoint()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "loader = DirectoryLoader(FILE_DIR, glob=\"**/*.txt\", show_progress=True)\n",
    "docs = loader.load()\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, collection_name=\"docstore\", persist_directory=DB_DIR)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 4\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = input(\"Enter query:\") #interactive Q&A\n",
    "\n",
    "ans = qa({\"query\": \"your query\"})\n",
    "\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
