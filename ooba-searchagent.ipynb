{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wrapper around Oobabooga APIs.\"\"\"\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "import requests\n",
    "from pydantic import Extra, root_validator\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "\n",
    "VALID_TASKS = (\"text2text-generation\", \"text-generation\")\n",
    "\n",
    "\n",
    "class OobaboogaEndpoint(LLM):\n",
    "    \"\"\"Wrapper around HuggingFaceHub Inference Endpoints.\n",
    "\n",
    "    To use, you should have the ``huggingface_hub`` python package installed, and the\n",
    "    environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass\n",
    "    it as a named parameter to the constructor.\n",
    "\n",
    "    Only supports `text-generation` and `text2text-generation` for now.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain.llms import HuggingFaceEndpoint\n",
    "            endpoint_url = (\n",
    "                \"https://abcdefghijklmnop.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "            )\n",
    "            hf = HuggingFaceEndpoint(\n",
    "                endpoint_url=endpoint_url,\n",
    "                huggingfacehub_api_token=\"my-api-key\"\n",
    "            )\n",
    "    \"\"\"\n",
    "    \n",
    "    endpoint_url: str = \"\"\n",
    "    \"\"\"Endpoint URL to use.\"\"\"\n",
    "    task: Optional[str] = None\n",
    "    \"\"\"Task to call the model with. Should be a task that returns `generated_text`.\"\"\"\n",
    "    model_kwargs: Optional[dict] = None\n",
    "    \"\"\"Key word arguments to pass to the model.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        _model_kwargs = self.model_kwargs or {}\n",
    "        return {\n",
    "            **{\"endpoint_url\": self.endpoint_url, \"task\": self.task},\n",
    "            **{\"model_kwargs\": _model_kwargs},\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"oobabooga_endpoint\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call out to HuggingFace Hub's inference endpoint.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                response = hf(\"Tell me a joke.\")\n",
    "        \"\"\"\n",
    "\n",
    "        # send request\n",
    "        request = {\n",
    "            'prompt': prompt,\n",
    "            'max_new_tokens': 256,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.1, #1.3\n",
    "            'top_p': 0.95, #0.1\n",
    "            'typical_p': 1,\n",
    "            'repetition_penalty': 1.0, #1.18\n",
    "            'top_k': 40,\n",
    "            'min_length': 0,\n",
    "            'no_repeat_ngram_size': 0,\n",
    "            'num_beams': 1,\n",
    "            'penalty_alpha': 0,\n",
    "            'length_penalty': 1,\n",
    "            'early_stopping': False,\n",
    "            'seed': -1,\n",
    "            'add_bos_token': True,\n",
    "            'truncation_length': 2048,\n",
    "            'ban_eos_token': False,\n",
    "            'skip_special_tokens': True,\n",
    "            'stopping_strings': stop + [\"\\nObservation:\"] #[]\n",
    "        }\n",
    "\n",
    "        response = requests.post(URI, json=request)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()['results'][0]['text']\n",
    "            if result.rsplit(None, 1)[1] == \"Observation\":\n",
    "                result = result.rsplit(None, 1)[0]\n",
    "            else:\n",
    "                result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "HOST = \"localhost:5000\"\n",
    "URI = f\"http://{HOST}/api/v1/generate\"\n",
    "\n",
    "endpoint_url = URI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OobaboogaEndpoint()\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "mathtool = load_tools([\"llm-math\"], llm=llm)\n",
    "tools = [DuckDuckGoSearchRun()]\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "# Now let's test it out!\n",
    "agent.run(\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Answer using the following format:\n",
    "\n",
    "Question: the original input question you must answer\n",
    "Thought: explain the question and what you think to do to answer the question\n",
    "Action: just the name of the selected tool from {tools}, i.e. DuckDuckGo Search\n",
    "Action Input: the input to the selected action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Now begin for real!\n",
    "\n",
    "Question: who won the 2023 liegi bikecycle race?\n",
    "Thought: \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
